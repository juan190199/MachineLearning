import warnings

import numpy as np


class QuadraticDiscriminantAnalysis(object):
    """
    Quadratic Discriminant Analysis: Classifier with a quadratic decision boundary,
    generated by fitting class conditional densities to the data using Bayes' rule

    The model fits a Gaussian density to each class

    Attributes
    -----------
    * covariance_: list of len n_classes of ndarray of shape (n_features, n_features) --
    For each class, gives the covariance matrix estimated using the samples of that class.
    Only present if `store_covariance` is True.

    * means_: array-like of shape (n_classes, n_features) -- Class-wise means

    * priors_: array-like of shape (n_classes,) -- Class priors

    * rotations_ : list of len n_classes of ndarray of shape (n_features, n_k) --
    For each class k an array of shape (n_features, n_k), where ``n_k = min(n_features, number of elements in class k)``
    It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to `V`,
    the matrix of eigenvectors coming from the SVD of `Xk = U S Vt`
    where `Xk` is the centered matrix of samples from class k.

    * scalings_ : list of len n_classes of ndarray of shape (n_k,) --
    For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the
    variance in the rotated coordinate system. It corresponds to `S^2 (n_samples - 1)`,
    where `S` is the diagonal matrix of singular values from the SVD of `Xk`,
    where `Xk` is the centered matrix of samples from class k.

    * classes_ : ndarray of shape (n_classes,) -- Unique class labels
    """

    def __init__(self, priors=None, reg_param=0., store_covariance=False, tol=1.0e-4):
        """
        :param priors: ndarray of shape (n_classes, ), default=None --
        Class priors. By default, the class priors are inferred from the training data
        :param reg_param: float, default=0.0 -- Regularizes the per-class covariance estimates
        :param store_covariance: bool, default=False --
        If True, the class covariance matrices are explicitely computed and stored in the `self.covariance_` attribute.
        :param tol: float, default=1.0e-4 -- Absolute threshold for a singular value to be considered significant
        """
        self.priors = np.asarray(priors) if priors is not None else None
        self.reg_param = reg_param
        self.store_covariance = store_covariance
        self.tol = tol

    def fit(self, X_train, y_train):
        """
        Fit the model according to the given design matrix and parameters

        :param self:
        :param X_train: array-like of shape (n_samples, n_features) --
        Design matrix, where n_samples is the number of samples and n_features is the number of features
        :param y_train: array-like of shape (n_samples, ) -- Target values
        :return:
        """
        self.classes_, y_train = np.unique(y_train, return_inverse=True)
        n_samples, n_features = X_train.shape
        n_classes = len(self.classes_)
        if n_classes < 2:
            raise ValueError('The number of classes has to be greater than one; got %d class' % (n_classes))

        if self.priors is None:
            self.priors_ = np.bincount(y_train) / float(n_samples)
        else:
            self.priors_ = self.priors

        cov = None
        store_covariance = self.store_covariance
        if store_covariance:
            cov = []
        means = []
        scalings = []
        rotations = []
        for idx in range(n_classes):
            Xg = X_train[y_train == idx, :]
            meang = Xg.mean(axis=0)
            means.append(meang)
            if len(Xg) == 1:
                raise ValueError(
                    'y_train has only one sample in class %s, covariance is ill defined.' % str(self.classes_[idx]))
            Xgc = Xg - meang
            # Xgc = U * S * V.T => 1/(n-1) * Xgc.T * Xgc = 1/(n-1) * V * (S ** 2) * V.T
            _, S, Vt = np.linalg.svd(Xgc, full_matrices=False)
            rank = np.sum(S > self.tol)
            if rank < n_features:
                warnings.warn('Variables are collinear')
            S2 = (S ** 2) / (len(Xg) - 1)
            S2 = ((1 - self.reg_param) * S2) + self.reg_param
            if self.store_covariance or store_covariance:
                cov.append(np.dot(S2 * Vt.T, Vt))
            scalings.append(S2)
            rotations.append(Vt.T)
        if self.store_covariance or store_covariance:
            self.covariance_ = cov
        self.means_ = np.asarray(means)
        self.scalings_ = scalings
        self.rotations_ = rotations
        return self

    def _decision_function(self, X_test):
        """
        Return log posterior

        :param X_test: array-like of shape (n_samples, n_features) -- Array of samples (test vectors)
        :return:
        """
        norm2 = []
        for i in range(len(self.classes_)):
            R = self.rotations_[i]
            S = self.scalings_[i]
            Xm = X_test - self.means_[i]
            X2 = np.dot(Xm, R * (S ** (-0.5)))
            norm2.append(np.sum(X2 ** 2, axis=1))
        norm2 = np.array(norm2).T  # Shape (X.shape[0], n_classes)
        u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])
        return -0.5 * (norm2 + u) + np.log(self.priors_)

    def decision_function(self, X_test):
        """
        Apply decision function to an array of samples.
        The decision function is equal (up to a constant factor) to the log-posterior of the model,
        i.e. `log p(y = k | x)`. In a binary classification setting this instead corresponds to the difference
        `log p(y = 1 | x) - log p(y = 0 | x)`.

        :param X_test: array-like of shape (n_samples, n_features) -- Array of samples (test vectors)
        :return: ndarray of shape (n_samples,) or (n_samples, n_classes) --
        Decision function values related to each class, per sample.
        In the two-class case, the shape is (n_samples,), giving the log likelihood ratio of the positive class.
        """
        dec_function = self._decision_function(X_test)
        # Handle special case of two classes
        if len(self.classes_) == 2:
            return dec_function[:, 1] - dec_function[:, 0]
        return dec_function

    def predict(self, X_test):
        """
        Perform classification on an array of test vectors X.

        :param X_test: array-like of shape (n_samples, n_features) -- Array of samples (test vectors)
        :return: ndarray of shape (n_samples,) -- Vector of predicted labels for each sample
        """
        d = self._decision_function(X_test)
        y_pred = self.classes_.take(d.argmax(axis=1))
        return y_pred

    def predict_proba(self, X_test):
        """
        Return posterior probabilities of classification

        :param X_test: array-like of shape (n_samples, n_features) -- Array of samples (test vectors)
        :return: ndarray of shape (n_samples, n_classes) -- Posterior probabilities of classification per class.
        """
        values = self._decision_function(X_test)
        # Compute likelihood of underlying Gaussian models
        likelihood = np.exp(values - values.max(axis=1)[:, np.newaxis])
        # Compute posterior probabilities
        return likelihood / likelihood.sum(axis=1)[:, np.newaxis]

    def predict_log_proba(self, X_test):
        """
        Return log of posterior probabilities of classification.

        :param X_test: array-like of shape (n_samples, n_features) -- Array of samples (test vectors)
        :return: ndarray of shape (n_samples, n_classes) -- Posterior log-probabilities of classification per class.
        """
        probas_ = self.predict_proba(X_test)
        return np.log(probas_)
